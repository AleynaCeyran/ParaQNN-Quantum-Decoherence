experiment:
  id: "mixed_regime_switching"
  version: "2.1.1"
  seeds: [42, 43, 44, 45, 46]
  device: "cuda"

data:
  path: "data/synthetic/mixed_regime_data.npz"
  test_split: 0.2
  shuffle: true
  num_workers: 0
  labels: "train_test_mask"

physics:
  # Units: microseconds (us), frequencies in MHz (consistent with manuscript conventions)
  time_span: 10.0
  total_samples: 50000

  # Regime boundaries (us)
  switch_t1: 4.0
  switch_t2: 7.0

  # System parameters
  drive_freq: 3.0
  detuning: 0.0
  t1_relaxation: 6.0
  t2_dephasing: 4.0

sampling:
  t_unit: "microseconds"
  # Uniform grid: t = linspace(0, time_span, total_samples)
  # Therefore: dt = time_span / (total_samples - 1)
  grid: "uniform"
  dt_definition: "dt_us = time_span / (total_samples - 1)"

noise:
  # Signal scale: population P(|1>) in approximately [0, 1]
  signal_scale: "population"

  # Pink noise: generated by spectral filtering of white noise,
  # normalized to unit variance and then scaled by pink_noise_std (population units).
  pink_noise_exponent: 1.0
  pink_noise_std: 0.06

  # RTN semantics are explicitly defined to eliminate ambiguity:
  # rtn_prob is a flip probability per time step (dimensionless), not a continuous-time rate.
  rtn_amplitude: 0.04
  rtn_prob: 0.005
  rtn_model: "flip_prob_per_step"

  # SPAM model: affine corruption before additive noise
  # P_noisy = (1 - 2e) * P_ideal + e ; then add (pink + rtn)
  spam_error: 0.02
  spam_model: "affine_population"

model:
  architecture: "ParaQNN"
  input_dim: 1
  output_dim: 1
  hidden_layers: 3
  neurons_per_layer: 128
  activation: "paraconsistent"
  initial_alpha: 5.0
  sharpness_k: 1.0

training:
  optimizer: "Adam"
  learning_rate: 0.001
  epochs: 4000
  batch_size: 512
  weight_decay: 1.0e-5
  scheduler:
    type: "ReduceLROnPlateau"
    patience: 200
    factor: 0.5

loss:
  lambda_signal: 1.0
  lambda_noise: 0.8
  lambda_contradiction: 0.5

checkpointing:
  save_dir: "checkpoints/mixed"
  save_best_only: true
  monitor: "val_loss"

